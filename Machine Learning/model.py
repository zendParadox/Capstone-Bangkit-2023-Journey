# -*- coding: utf-8 -*-
"""TF Serving_SavedModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X92A2VRR7Vs8SbuIdaCW3Y2IU4Cg09b7
"""

pip install autokeras

import tensorflow as tf
import numpy as np
import csv
from sklearn.model_selection import train_test_split
import pickle

from tensorflow import keras
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

filename = "" # change to your dataset path

attributes = ["disability_type", "skills_one", "skills_two", "position"]

data = []

with open(filename, newline='') as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
        item = {}
        for attribute in attributes:
            item[attribute] = row[attribute]
        data.append(item)

# Split the data into training set and test set
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

# Print the training set
print("Training set:")
for item in train_data:
    print(item)

# Print the test set
print("Test set:")
for item in test_data:
    print(item)

# Extract the text features from the data
train_text = np.array([data['disability_type'] + ' ' + data['skills_one'] + ' ' + data['skills_two'] + ' ' + data['position'] for data in train_data])
test_text = np.array([data['disability_type'] + ' ' + data['skills_one'] + ' ' + data['skills_two'] + ' ' + data['position'] for data in test_data])

# Convert the labels to numerical values
train_labels = [data['position'] for data in train_data]
test_labels = [data['position'] for data in test_data]

# Convert the labels to one-hot encoded vectors
unique_labels = list(set(train_labels + test_labels))
num_labels = len(unique_labels)

label_mapping = {label: i for i, label in enumerate(unique_labels)}
train_labels_encoded = [label_mapping[label] for label in train_labels]
test_labels_encoded = [label_mapping[label] for label in test_labels]

train_labels_encoded = np.array(train_labels_encoded, dtype=np.int32)
test_labels_encoded = np.array(test_labels_encoded, dtype=np.int32)

train_labels_one_hot = tf.keras.utils.to_categorical(train_labels_encoded, num_labels)
test_labels_one_hot = tf.keras.utils.to_categorical(test_labels_encoded, num_labels)

# Tokenize the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_text)

train_sequences = tokenizer.texts_to_sequences(train_text)
test_sequences = tokenizer.texts_to_sequences(test_text)

# Save the tokenizer
with open('tokenizer.pkl', 'wb') as tokenizer_file:
    pickle.dump(tokenizer, tokenizer_file)

# Pad sequences to ensure equal length
max_seq_length = 100
train_sequences = pad_sequences(train_sequences, maxlen=max_seq_length)
test_sequences = pad_sequences(test_sequences, maxlen=max_seq_length)

# Define the early stopping criteria
from tensorflow.keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(
    monitor='val_loss',  # Monitor validation loss
    min_delta=0.001,  # Minimum change to consider as improvement
    patience=5,  # Number of epochs with no improvement to stop training
    restore_best_weights=True,  # Restore the best weights recorded during training
)

# Define the model architecture
embedding_dim = 320  # Define the dimensionality of the word embeddings

model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_seq_length))
model.add(LSTM(224))
model.add(Dense(num_labels, activation='softmax'))

# Compile the model
from tensorflow.keras.optimizers import Adam
optimizer = Adam(learning_rate=0.01)
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

# Train the model
batch_size = 32
epochs = 20

model.fit(train_sequences, train_labels_one_hot, batch_size=batch_size, epochs=epochs, validation_data=(test_sequences, test_labels_one_hot), callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(test_sequences, test_labels_one_hot)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy:.4f}')

# Load the tokenizer
with open('tokenizer.pkl', 'rb') as tokenizer_file:
    tokenizer = pickle.load(tokenizer_file)

# Recommender System
import requests
import json

# Set the input data
input_data = {
  "signature_name": "serving_default",
  "instances": [
    {
      "disability_type": "Hearing Impairment",
      "skills_one": "Adobe Illustrator",
      "skills_two": "UI UX Design",
      "position": "Unknown"
    }
  ]
}

new_text = [input_data['instances'][0]['disability_type'] + ' ' + input_data['instances'][0]['skills_one'] + ' ' + input_data['instances'][0]['skills_two'] + ' ' + input_data['instances'][0]['position']]

new_sequences = tokenizer.texts_to_sequences(new_text)
new_sequences = pad_sequences(new_sequences, maxlen=max_seq_length)

import csv

# Save unique labels to a CSV file
with open('unique_labels.csv', 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['label'])
    for label in unique_labels:
        writer.writerow([label])

import os

# Save the model in TensorFlow's SavedModel format
model.save('/content/sample_data/my_model')
import tensorflow as tf


# If you want to save it in a specific version, use this:
version = 1
export_path = os.path.join('/content/sample_data/my_model', str(version))
print('export_path = {}\n'.format(export_path))

tf.keras.models.save_model(
    model,
    export_path,
    overwrite=True,
    include_optimizer=True,
    save_format=None,
    signatures=None,
    options=None
)

print('\nSaved model:')
!ls -l {export_path}
# Load the SavedModel
loaded_model = tf.saved_model.load('/content/sample_data/my_model')

# Convert the model to a callable function
model_fn = loaded_model.signatures['serving_default']

# Assuming `new_sequences` is your input data
input_data = tf.convert_to_tensor(new_sequences, dtype=tf.float32)

# Make predictions using the model function
predictions = model_fn(input_data)

# Access the prediction output
output = predictions['dense']

# Assuming `new_sequences` is your input data
input_data = tf.convert_to_tensor(new_sequences, dtype=tf.float32)
predictions = loaded_model(input_data)

# Now you can use `predictions` as you did before

predictions = model.predict(new_sequences)
predicted_label_indices = np.argsort(predictions[0])[::-1][:10]
ranked_labels = [unique_labels[index] for index in predicted_label_indices]

# Sequential output according to rank
output = "[" + ", ".join(ranked_labels) + "]"
print(output)

from google.colab import files

# Specify the path of the SavedModel directory
saved_model_path = "/content/sample_data/my_model"

# Create a zip file of the SavedModel directory
!zip -r saved_model.zip {saved_model_path}

# Download the zip file
files.download("saved_model.zip")